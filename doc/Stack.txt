Note: This description is deliberately vague about how the stack is physically assembled, how it grows and shrinks, etc.  There are still competing concepts how to arrange the stack, so the discussion that follows here is meant to be universally applicable to all underlying implementations.  For quick reference, some ideas are:

- The stack is an array implemented using a paged data structure, similar to any other segment.  This is good because it shares machinery with all other segment processing, and provides a simple, flat address space to work in.  This is less good because it does not support any kind of state sharing between tasks that share a common execution context.

- The stack is a linked list where each portion of stack references the portion that came before.  Because the stack grows and shrinks from the same end, working with a linked list may be more efficient than a fully paged structure.  Also, tasks that share a common base context may share the portion of stack that "came before", similar to the saguaro stack found in the Burroughs large systems of yore.  There are issues with jumping over the in-addressable parts of the stack that contain the pointers to each other, and there are inefficiencies when moving between pages of the stack as there is no direct page table to follow.

The primary workspace of a task is its stack.  A task makes use of its RAM-based parameter stack to store automatic variables across call frames.  A task also uses the register-based operand stack to provide parameters to CPU instructions and get results back.  To protect from return-oriented programming and other stack manipulation shenanigans, the stacks are guarded to ensure orderly use.

The parameter stack is indexed in reference to a movable partition that separates the accessible portion of the stack from the protected portion.  The stack grows in a positive direction, so addresses beneath the partition are inaccessible, and addresses at and above the partition are accessible via positive indices.  Thus a function may place its local variables at known offsets within the stack and be able to use a consistent indexing scheme to find them.  While a compiler will handle this automatically, it is helpful to a human assembly programmer to not have to mentally adjust local variable offsets throughout a function.

Because the overall CPU architecture is strongly segmented, it is not possible for the stack to smash into anything else.  Either the stack will fill up, leading to faults when attempting to push additional items, or the system will run out of memory, leading to faults when attempting to grow the stack.

To prevent manipulation of the return address, the return address is hidden just past the partition when executing a call.  As part of the call, the caller will tell the CPU an offset to use for the return address, and the CPU will move the floor just past that offset during the calling process.  Because each call frame can access any address past the partition, the caller can easily set up the environment for the callee.  Once the call takes place, the callee cannot manipulate its own return address, nor can the callee access the local state of the caller.

--- stack grows this way --->
val val val return_address old_floor param param ...
^           ^                        ^
|           |                        |
|           |                        +-- floor will end up here after call
|           +-- caller specifies this offset (keeps all offsets positive)
+-- floor is here before the call

Note: This eliminates the possibility of inout or reference parameters.  Because each frame is a forced new context, the best we can do is copy return values back at each frame.  This seems like a rather substantial performance downside, so it is worth considering how to improve the situation.  A few ideas exist so far:

- Use a second stack for call/return, independent of parameters.  There could still be a partition in the parameter stack, but there would not be an obligation to move it.  The call/return stack would not need a separate partition because it is not addressable from user code.  This has the downside to requiring additional overhead during context switches.  A single stack needs to only save and restore the partition register (and restore the base pointer), but a second stack needs to save and restore an additional stack pointer.

- Use a tagged value to indicate a return address.  This idea isn't very mature, because the return still needs a way to find the return address, and without another pointer, this doesn't seem to work well. Eliminating the partition and using that register as a pointer to the return address keeps about the same performance but loses the protection of the partition.

- Since a theme of this CPU is the passing of memory segments, the loss of inout may be an acceptable consequence.  Still, it is worth attempting to find a way to make this work.

The operand stack is a moderately sized stack of registers that provide inputs to instructions and the results from instructions.  The operand stack can monitor its own fullness and cause faults if, for example, an attempt is made to push a new value when the stack is already full.  Thus code must ensure to empty out the stack of unnecessary values, rather than assume they will push off the end and be lost.

To provide for quick interrupts, it may be necessary to give the system access to additional space in the operand stack that is otherwise inaccessible to user code.  This has not been determined yet, because certain aspects of task switching are still unknown.

Another proposal was to combine the parameter and operand stacks into one, implemented as a set of registers that spill to RAM once full, and shadow to RAM opportunistically.  On any instruction that does not otherwise involve RAM access, use that cycle to write back any dirty registers to RAM.  This is similar to the way the stack works on RPL systems.  However, it is important to note that RPL provides structured error handling for the stack emptying unexpectedly, and also provides no protection from an RPL program that simply consumes the entire stack and does nothing useful.  In a context where a caller only somewhat trusts the callee, making the stack a free-for-all is imprudent.  Another big issue is the lack of straightforward addressing for values deeper than the top few stack items.  For an operand stack, this is not a problem, as those are ephemeral values, but when mixed with all local variables, the need to access deeper into the stack with constant time is critical.  Finally, mingling the two makes it harder to reason about the indices of local variables, making it more error-prone to code assembly.  For all these reasons, this proposal has largely been rejected in current thinking.

Functions can have flexible calling conventions whether to have parameters and return values in the parameter stack or the operand stack.  This can also give something like inout parameters to a limited extent, as each frame can accept parameters using the operand stack, modify them, and leave them there for the calling frame.  The limited size of the operand stack means this cannot be done for large numbers of parameters, and likely not across large chains of calls, but should be workable for a value or two across a small number of calls.  The most generic calling convention will be to place everything in the parameter stack, but something like a compiler tag on a parameter can aim to keep it in the operand stack, like the "register" keyword in C.
